/*
 * FeedForward.cu
 *
 *  Created on: Jan 28, 2018
 *      Author: joseph
 */

#ifndef GRU_UNIT
#define GRU_UNIT

#include "Layer.h"
#include <mutex>

namespace BC {
namespace NN {
template<class derived>
struct GRU : public Layer<derived> {

/*
 *  TESTED AND APPROVED
 */

public:

	using Layer<derived>::sum_gradients;
	using Layer<derived>::zero;
	using Layer<derived>::xs;
	using Layer<derived>::lr;

	omp_unique<mat> wz_gradientStorage, wf_gradientStorage; //gradient storage weights
	omp_unique<mat> rz_gradientStorage, rf_gradientStorage;	//gradient storage recurrent weights
	omp_unique<vec> bz_gradientStorage, bf_gradientStorage; //gradienst storage bias

	omp_unique<vec> c,  f,  z;
	omp_unique<vec> dc, df, dz;
	bp_list<vec> 	ys, fs, zs;							//storage for outputs

	mat wz, wf;
	mat rz, rf;
	vec bz, bf;

	auto& xs() { return this->prev().ys(); }	//get the storage for inputs

	GRU(int inputs) :
			Layer<derived>(inputs),
			wz(this->OUTPUTS, this->INPUTS),
			rz(this->OUTPUTS, this->OUTPUTS),
			bz(this->OUTPUTS),
			wf(this->OUTPUTS, this->INPUTS),
			rf(this->OUTPUTS, this->OUTPUTS),
			bf(this->OUTPUTS)
		{
		rz.randomize(-4, 0);
		wz.randomize(-4, 4);
		bz.randomize(-4, 4);
		rf.randomize(-4, 0);
		wf.randomize(-4, 4);
		bf.randomize(-4, 4);
		init_storages();

	}


	auto forwardPropagation(const vec& x) {
		xs().push(x);

		//ensure theres valid inital cell state to train on
		if (ys().isEmpty()) {
			ys().push(c());
			fs().push(f());
			zs().push(z());
		}

		c() = g(wz * x + rz * c() + bz);
		return this->next().forwardPropagation(c());
	}
	auto backPropagation(const vec& dy) {
		vec& x = xs().first();				//load the last input
		vec& y = ys().second();				//load last and remove
		ys().pop();							//update

		wz_gradientStorage() -= dy   * x.t();
		rz_gradientStorage() -= dc() * y.t();
		bz_gradientStorage() -= dy;

		dc() += dy;							//add the error

		return this->prev().backPropagation(wz.t() * dy % gd(x));
	}
	auto forwardPropagation_Express(const vec& x) const {
		c() = g(wz * x + rz * c() + bz);
		return this->next().forwardPropagation_Express(c());
	}

	void updateWeights() {
		//sum all the gradients
		wz_gradientStorage.for_each(sum_gradients(wz, lr));
		rz_gradientStorage.for_each(sum_gradients(rz, lr));
		bz_gradientStorage.for_each(sum_gradients(bz, lr));
		wz_gradientStorage.for_each(sum_gradients(wz, lr));
		rz_gradientStorage.for_each(sum_gradients(rz, lr));
		bz_gradientStorage.for_each(sum_gradients(bz, lr));

		this->next().updateWeights();
	}

	void clearBPStorage() {
		wz_gradientStorage.for_each(zero);	//gradient list
		rz_gradientStorage.for_each(zero);	//gradient list
		bz_gradientStorage.for_each(zero);	//gradient list

		dc.for_each([](auto& var) { var.zero(); }); 	//gradient list
		ys.for_each([](auto& var) { var.clear();});		//bp_list

		this->next().clearBPStorage();
	}
	void set_omp_threads(int i) {
		ys.resize(i);
		dc.resize(i);
		c.resize(i);
		wz_gradientStorage.resize(i);
		bz_gradientStorage.resize(i);
		rz_gradientStorage.resize(i);

		init_storages();
		this->next().set_omp_threads(i);
	}

	void write(std::ofstream& is) {
	}
	void read(std::ifstream& os) {
	}
	void init_storages() {
		//for each matrix/vector gradient storage initialize to correct dims
		wz_gradientStorage.for_each([&](auto& var) { var = mat(this->OUTPUTS, this->INPUTS);  var.zero(); });
		rz_gradientStorage.for_each([&](auto& var) { var = mat(this->OUTPUTS, this->OUTPUTS); var.zero(); });
		bz_gradientStorage.for_each([&](auto& var) { var = vec(this->OUTPUTS);			     var.zero(); });

		//for each cell-state error initialize to 0
		dc.for_each([&](auto& var) { var = vec(this->OUTPUTS); var.zero(); });
		c.for_each([&](auto& var) { var = vec(this->OUTPUTS); var.zero(); });

	}
};

}
}


#endif /* FEEDFORWARD_CU_ */
